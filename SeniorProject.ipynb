{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project is about extracting pattern from the guest comments, later predicting the new guest behavior based on the extracted patterns. To do that, I have used several python libraries; NTLK, Gensim, Scikit-learn. The data is from booking.com which 515,000 guest reviews and scoring of 1493 luxury hotels across Europe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "# read the data\n",
    "reviews = pd.read_csv(\"/home/han/jnotebooks/data/Hotel_Reviews.csv\")\n",
    "# reviews.head() # see the data\n",
    "# append the positive and negative reviews. add as a label\n",
    "reviews[\"review\"] = reviews[\"Positive_Review\"] + reviews[\"Negative_Review\"]\n",
    "# create the label as good or bad \n",
    "reviews[\"good_bad\"] = reviews[\"Reviewer_Score\"].apply(lambda x: 1 if x > 5 else 0)\n",
    "# select review and good_bad label\n",
    "reviews = reviews[[\"review\", \"good_bad\"]]\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data\n",
    "**DataFrame.sample** function will be used to return a random sample of items from an asis of object. Reason is to speed up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frac: Fraction of axis items to return. \n",
    "# replace: Allow or disallow sampling of the same row more than once.\n",
    "reviews = reviews.sample(frac = 0.1, replace = False, random_state=0)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data\n",
    "**Positive_Review:** Positive Review the reviewer gave to the hotel. If the reviewer does not give the positive review, then it should be: 'No Positive'\n",
    "\n",
    "**Negative_Review:** Negative Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'No Positive' or 'No Negative' has no meaning in the reviews, so these words are removed. \n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet** is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token** Each “entity” that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is “tokenized” into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. \n",
    "\n",
    "**e.g.** the process of reducing the different forms of a word to one single form, for example, reducing \"builds\", \"building\", or \"built\" to the lemma \"build\"\n",
    "\n",
    "*In lemmatisation,''nt\" is marked as \"not\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop Words:** A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part-of-speech tagging** (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,[1] based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def getCleanText(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize the text and remove punctuation. \n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove the words that contains number\n",
    "    text = [word for word in text if not any(char.isdigit() for char in word)]\n",
    "    # remove stop words\n",
    "    text = [x for x in text if x not in stopwords.words(\"english\")]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text. e.g. [('now', 'RB'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], getWordnetPos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def getWordnetPos(pos_tag):\n",
    "    if pos_tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "    \n",
    "# clean text data\n",
    "reviews[\"clean_review\"] = reviews[\"review\"].apply(lambda x: getCleanText(x))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Firstly, sentiment analysis going to be added, because it is directly related with guests' review. To make sentiment analysis, Vader is going to be used from nltk. \n",
    "\n",
    "**VADER (Valence Aware Dictionary and sEntiment Reasoner)** is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labelled according to their semantic orientation as either positive or negative. Moreover it considers the context of the sentences. Vader returns; neutrality score, positivity score, negativity score, overall score for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# polarity_scores method of SentimentIntensityAnalyzer\n",
    "# oject gives a sentiment dictionary. \n",
    "# which contains pos, neg, neu, and compound scores. \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "reviews[\"sentiments\"] = reviews[\"review\"].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.). The axis labels are collectively called index.\n",
    "reviews = pd.concat([reviews.drop([\"sentiments\"], axis=1), reviews[\"sentiments\"].apply(pd.Series)], axis=1)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of characters label\n",
    "reviews[\"number_chars\"] = reviews[\"review\"].apply(lambda review: len(review))\n",
    "# number of words label\n",
    "reviews[\"number_words\"] = reviews[\"review\"].apply(lambda review: len(review))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Gensim is going to be used to extract vector representations of reviews.\n",
    "\n",
    "**Gensim** is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n",
    "\n",
    "**Word2vec** is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n",
    "\n",
    "**The purpose** and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words.\n",
    "\n",
    "**Doc2Vec** is a Model that represents each Document as a Vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Initialize & train a model\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews[\"clean_review\"].apply(lambda x: x.split(\" \")))]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "# transfrom each document into a vector data\n",
    "# Infer a vector(infer_vector) for given post-bulk training document.\n",
    "dataframe_doc2vec = reviews[\"clean_review\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "dataframe_doc2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_doc2vec.columns = [\"doc2vec_\" + str(x) for x in dataframe_doc2vec.columns]\n",
    "dataframe_doc2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.concat([reviews, dataframe_doc2vec], axis=1)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the TF-IDF values are going to added for every word and every document.\n",
    "\n",
    "In information retrieval, **tf–idf** or **TFIDF**, short for **term frequency–inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "**The reason of using TF-IDF:** It considers the importance of words in text, rare words my have more importance than common words.\n",
    "\n",
    "TF-IDF columns are going to added for each word that appear in minimum 10 different texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idfs labels \n",
    "# fit_transform: Learn vocabulary and idf, return document-term matrix.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=10)\n",
    "tfidf_result = tfidf.fit_transform(reviews[\"clean_review\"]).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = reviews.index\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.concat([reviews, tfidf_df])\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"good_bad\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forests** is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ignored_columns = [\"good_bad\", \"review\", \"clean_review\"]\n",
    "labels = [column for column in reviews.columns if column not in ignored_columns]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews[labels], reviews[\"good_bad\"], test_size=0.3, random_state=0)\n",
    "\n",
    "# train\n",
    "random_forest = RandomForestClassifier(n_estimators = 100, random_state=0).fit(x_train, y_train)\n",
    "\n",
    "# predict probablity \n",
    "predicted_probability = [r[1] for r in random_forest.predict_proba(x_test)]\n",
    "\n",
    "# feature importance\n",
    "feature_importances = pd.DataFrame({\"feature\": labels, \"importance\": random_forest.feature_importances_}).sort_values(\"importance\", ascending = False)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier is used to see the feature impotances. As seen in the head, sentiment analyis is provides the most important features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tpr, fpr = roc_curve(y_test, predicted_probability, pos_label=1)\n",
    "\n",
    "plt.figure(1, figsize=(16, 9))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Area under the ROC curve = %0.2f' % auc(fpr, tpr))\n",
    "plt.plot([0,1], [0,1], lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is mostly the best to show the quality of classifier. In general, an AUC of 0.5 suggests no discrimination (i.e., ability to diagnose patients with and without the disease or condition based on the test), 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding. As seen the the curve, the model is very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted_probability)\n",
    "precision, recall, _ = precision_recall_curve(y_test, predicted_probability)\n",
    "\n",
    "plt.figure(1, figsize = (16, 9))\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {}))\n",
    "\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in 'explore data' section, the data set is out of balance. Therefore PR curve is the best metric for this data set. As seen in the graph, precision decreases when we increase the recall. This means prediction threshold adaption is required. To reach high recall values, low prediction treshold values are needed. In contrast, to reach high precison value, high prediction treshold values are needed. Thus the graph represents that the model has a well predictive capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, guest reviews has been used to predict the guest behaviour via sentiment analyisis, natural language processing features, gensim topic modelling, term frequency–inverse document frequency values. Therefore it is possible to make sentiment analyisis and make prediction form the raw guest reviews."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
